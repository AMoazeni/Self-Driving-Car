{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<br></br>\n",
    "Take me to the [code]() for the Self Driving Car!\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Research and development of autonomous vehicles has accelerated in the past decade, due to advances in computing speed, sensor technology, and popular interest. This article explores the Software Architecture for a Self-Driving Car.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "The controller uses a Model Predictive Control (MPC) algorithm to anticipate the car's future position, knowing the car's Vehicle Dynamics equations and measured position (state).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Self-Driving-Car/master/Jupyter%20Notebook/Images/01%20-%20Car%20Sensors.png\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "This software architecture shown above has been tested on the self-driving Hyundai Sonata shown in the picture. It has been successfully tested in highway and city driving conditions.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "The algorithm technically works on any [Drive-By-Wire](https://en.wikipedia.org/wiki/Drive_by_wire) car which has electronically controlled steering wheel, gas, and brake pedals. You can read sensor values (Camera, Radar, Lidar, GPS) and control the car directly from the car's [CAN Bus](https://en.wikipedia.org/wiki/CAN_bus), check with the manufacturer to confirm.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Model Predictive Control (MPC) Algorithm\n",
    "\n",
    "<br></br>\n",
    "The main components of a modern autonomous vehicle are\n",
    "localization, perception, and control. This architecture lets you control the vehicle acceleration, brake, and steering using Model Predictive Control (MPC).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Self-Driving-Car/master/Jupyter%20Notebook/Images/02%20-%20Control%20System.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "At each sampling time step (beginning at the current state), an open loop optimal control problem is solved over a finite horizon. For each consecutive time step, time step a new optimal control problem based on new measurements is solved over a shifted horizon.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "The optimal solution relies on a dynamic model of the process with respects to input constraints, output constraints, and minimizing a performance index (cost). The cost equation for this model is the simple distance formula, this keeps the car at the center of the lane by minimizing cost error (car's center position coordinate minus lane center coordinate).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Self-Driving-Car/master/Jupyter%20Notebook/Images/03%20-%20MPC%20Algorithm.png\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Vehicle Dynamic\n",
    "\n",
    "<br></br>\n",
    "Here is a description of a car's Vehicle Dynamics equation. You can find the constants for your car with some simple research or taking measurements.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Self-Driving-Car/master/Jupyter%20Notebook/Images/04%20-%20Vehicle%20Dynamics.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Self-Driving-Car/master/Jupyter%20Notebook/Images/05%20-%20Vehicle%20Equations.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Self-Driving-Car/master/Jupyter%20Notebook/Images/06%20-%20Vehicle%20Constants.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "The only input considered is the vehicle steering angle (steering wheel) and acceleration (gas and brake pedal). Some constraints are implemented to smooth out driving commands like a limited steering angle range, and limited rate of change for steering angle (reduces jerk in steering). \n",
    "\n",
    "\n",
    "<br></br>\n",
    "The simulated vehicle is modeled to track a circle trajectory, but can follow any trajectory. Your real-world trajectory comes from lane detection which is generated by your camera sensor's Computer Vision (CV) system. The Vehicle Model equations need to be [linearized](https://apmonitor.com/pdc/index.php/Main/ModelLinearization) which is like finding the next position of a point given its initial position and slope. Linearized equations are MUCH easier to calculate from a computer's perspective.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Self-Driving-Car/master/Jupyter%20Notebook/Images/07%20-%20Vehicle%20Linear.png\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Bicycle Model\n",
    "\n",
    "<br></br>\n",
    "Bicycle models are not as computationally expensive as car models, and also less prone to errors due to their simplicity. It's highly recommended to use a Bicycle Model for the real Self-Driving Car. Check out this research paper which compares [Vehicle versus Bicycle Models for Autonomous Vehicles]().\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Autonomous Drifting!\n",
    "\n",
    "<br></br>\n",
    "The tire model equation is not necessary for normal driving. It's only included for fun to simulate drifting. It forces tire friction to saturate (no traction) and get the car to drift! Remove the tire equations and friction saturation constraints to simulate regular driving.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Results\n",
    "\n",
    "The following images are made in MATLAB, they show the lane (Red lines), the car and steering wheel angle (Black box), as well as the covered trajectory (Blue line).\n",
    "\n",
    "### Normal Driving\n",
    "\n",
    "\n",
    "### Drifting\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Robot Operating System (ROS)\n",
    "\n",
    "<br></br>\n",
    "The [Robot Operating System (ROS)](http://www.ros.org/) is an open source platform that's quickly becoming an industry standard in the robotics field. ROS let's you quickly prototype and reuse code through their robust Publisher-Subscriber ([pub-sub](https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern)) architecture. You can also use different languages to create your nodes which is a huge bonus for modular design.\n",
    "\n",
    "<br></br>\n",
    "The only downside to ROS is that it's a Soft Real-Time system which is fine for single projects but becomes problematic when implemented at scale. There are workarounds for getting Hard Real-Time performance, but that's a topic for another post. Follow along with these [ROS Tutorials](http://wiki.ros.org/ROS/Tutorials) to get you started, section 1.1 steps 1-13 are the most important tutorials.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Code\n",
    "\n",
    "<br></br>\n",
    "This repository only contains the MATLAB simulation for this project. However in the real world, this project was implemented on the Self-Driving Car using Robot Operating System (ROS). Find the MATLAB simulation in the 'Code' folder of this repository.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "```shell\n",
    "$ git clone https://github.com/AMoazeni/Machine-Learning-Image-Recognition.git\n",
    "$ cd Machine-Learning-Image-Recognition\n",
    "```\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 1999 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Convolution Neural Network\n",
    "# Part 1 - Building CNN Architecture and Import Data\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "# 'Sequential' library used to Initialize NN as sequence of layers (Alternative to Graph initialization)\n",
    "from keras.models import Sequential\n",
    "# 'Conv2D' for 1st step of adding convolution layers to images ('Conv3D' for videos with time as 3rd dimension)\n",
    "from keras.layers import Conv2D\n",
    "# 'MaxPooling2D' step 2 for pooling of max values from Convolution Layers\n",
    "from keras.layers import MaxPooling2D\n",
    "# 'Flatten' Pooled Layers for step 3\n",
    "from keras.layers import Flatten\n",
    "# 'Dense' for fully connected layers that feed into classic ANN\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initializing the CNN\n",
    "# Calling this object a 'classifier' because that's its job\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "# Apply a method 'add' on the object 'classifier'\n",
    "# Filter = Feature Detector = Feature Kernel\n",
    "# 'Conv2D' (Number of Filters, (Filter Row, Filter Column), input shape of inputs = (3 color channels, 64x64 -> 256x256 dimension of 2D array in each channel))\n",
    "# Start with 32 filters, work your way up to 64 -> 128 -> 256\n",
    "# 'input_shape' needs all picture inputs to be the same shape and format (2D array for B&W, 3D for Color images with each 2D array channel being Blue/Green/Red)\n",
    "# 'input_shape' parameter shape matters (3,64,64) vs (64,64,3)\n",
    "# 'Relu' Rectifier Activation Function used to get rid of -ve pixel values and increase non-linearity\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "# Reduces the size of the Feature Map by half (eg. 5x5 turns into 3x3 or 8x8 turns into 4x4)\n",
    "# Preserves Spatial Structure and performance of model while reducing computation time\n",
    "# 'pool_size' at least needs to be 2x2 to preserve Spatial Structure information (context around individual pixels)\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolution layer to improve performance\n",
    "# Only need 'input_shape' for Input Layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "# Take all the Pooled Feature Maps and put them into one huge single Vector that will input into a classic NN\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "# Add some fully connected hidden layers (start with a number of Node between input and output layers)\n",
    "# [Input Nodes(huge) - Output Nodes (2: Cat or Dog)] / 2 = ~128?...\n",
    "# 'Activation' function makes sure relevant Nodes get a stronger vote or no vote\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "# Add final Output Layer with binary options\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "# 'adam' Stochastic Gradient Descent optimizer\n",
    "# 'loss' function. Logarithmic loss for 2 categories use 'binary_crossentropy' and 'categorical_crossentropy' for more objects\n",
    "# 'metric' is the a performance metric\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create random transformation from Data to increase Dataset and prevent overfitting\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# 'batch_size' is the number of images that go through the CNN every weight update cycle\n",
    "# Increase 'target_size' to improve model accuracy \n",
    "\n",
    "training_set = train_datagen.flow_from_directory('../Data/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('../Data/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 2679s 335ms/step - loss: 0.4028 - acc: 0.8063 - val_loss: 0.5167 - val_acc: 0.8038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10fe0fe48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# Increase 'epochs' to boost model performance (takes longer)\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 8000,\n",
    "                         epochs = 1,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Save model to file\n",
    "# Architecture of the model, allowing to reuse trained models\n",
    "# Weights of the model\n",
    "# Training configuration (loss, optimizer)\n",
    "# State of the optimizer, allowing to resume training exactly where you left off\n",
    "classifier.save('../Data/saved_model/CNN_Cat_Dog_Model.h5')\n",
    "\n",
    "# Examine model\n",
    "classifier.summary()\n",
    "\n",
    "# Examine Weights\n",
    "classifier.weights\n",
    "\n",
    "# Examine Optimizer\n",
    "classifier.optimizer\n",
    "\n",
    "\n",
    "\n",
    "# Load saved Model\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('../Data/saved_model/CNN_Cat_Dog_Model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model class indices are: {'cats': 0, 'dogs': 1}\n",
      "\n",
      "Prediction: dog\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "\n",
    "# Place a new picture of a cat or dog in 'single_prediction' folder and see if your model works\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('../Data/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "# Add a 3rd Color dimension to match Model expectation\n",
    "test_image = image.img_to_array(test_image)\n",
    "# Add one more dimension to beginning of image array so 'Predict' function can receive it (corresponds to Batch, even if only one batch)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "# We now need to pull up the mapping between 0/1 and cat/dog\n",
    "training_set.class_indices\n",
    "# Map is 2D so check the first row, first column value\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "# Print result\n",
    "\n",
    "print(\"The model class indices are:\", training_set.class_indices)\n",
    "\n",
    "print(\"\\nPrediction: \" + prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
